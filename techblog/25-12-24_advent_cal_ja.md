# GenJAX: プログラマブル推論を備えたプロバビリスティック・プログラミング

## はじめに

プロバビリスティック・プログラミングは、不確実性の下で推論を行うモデルを構築するための強力なパラダイムとして台頭してきました。従来の確率的機械学習では、実務者は通常、推論アルゴリズムを手作業で設計していました—これは深い数学的専門知識を必要とする、労力がかかりエラーを起こしやすいプロセスです。プロバビリスティック・プログラミングはこれを変えます：複雑な推論アルゴリズムを手作業でコーディングする代わりに、実務者はモデルをプログラムとして表現し、数学的な重労働をシステムに任せることができます。しかし、ここに課題があります：汎用的な推論は往々にして遅すぎる一方で、手作業でチューニングしたアルゴリズムは実装と保守が煩雑です。

そこで登場するのが**Gen**です。MITのプロバビリスティック・コンピューティングプロジェクトによって開発されたこのプロバビリスティック・プログラミングシステムは、表現力豊かなモデリングとプログラマブルな推論という両方の利点を提供します。

![Probabilistic Programs as the intersection of Symbolic Languages, Probabilistic Models, and Neural Networks](assets/probabilistic-programs-venn.png)
*出典: https://jpcca.org/probcomp

## Genとは何か？

Genは、推論アルゴリズムの難解な数学と低レベルの実装詳細を自動化しながら、ユーザーに特定のモデルに合わせたソリューションをカスタマイズする柔軟性を与えるオープンソースのプロバビリスティック・プログラミングシステムです。

もともとJuliaで構築された（[Gen.jl](https://github.com/probcomp/Gen.jl)として）Genは、確率的分岐、ループ、再帰を含む通常のコードを使用して生成モデルを記述でき、効率的な推論アルゴリズムを構築するための構成要素を提供します。

### 主な機能：

- **プログラマブル推論**：モデルの構造に合わせた推論アルゴリズム（MCMC、SMC、変分推論）を合成—離散変数にはギブスサンプリング、連続変数にはHMCを混合可能
- **ハイブリッド推論**：ニューラルネットワークの提案分布と原理的なベイズ推論を組み合わせ（本記事の範囲外ですが、複雑なドメインへのスケーリングにおける重要な機能）
- **確率的構造**：潜在変数の数が未知である問題（例：物体検出、未知のKを持つクラスタリング）をモデル化
- **プログラマブルなトレードオフ**：プロトタイピングには高レベルの抽象化を使用し、本番環境では最適化されたコードにドロップダウン
- **クリーンなAPI**：自動微分と推論プリミティブを利用

## GenJAXの登場：Python向けGPUアクセラレーテッドベイズ推論

**GenJAX**は、GenのコアとなるアブストラクションをJAX上で直接再実装し、本番AIが求めるパフォーマンス特性を持つ原理的な不確実性定量化をPythonにもたらします。すでにPythonエコシステムで作業している実務者にとって、これはスタックを離れることなくプログラマブルな推論が可能になることを意味します—JAXの強みを活用して：

- **GPUアクセラレーション**：推論計算のJITコンパイルとGPUデバイスへの自動バッチ処理
- **JAX変換**：`jit`、`vmap`、自動微分との完全な互換性
- **馴染みのある構文**：`@gen`デコレータを使用してPythonで生成関数を定義
- **組み込みアルゴリズム**：サンプリング重要度リサンプリング（SIR）やその他の推論手法をすぐに利用可能

### クイックサンプル

GenJAXでは、生成関数（構造化されたサンプル空間上の確率測度を表す計算オブジェクト）を定義します。これらの関数は合成、トレース、様々な推論アルゴリズムとの使用が可能です：

```python
@gen
def my_model():
    slope = normal(0.0, 1.0) @ "slope"
    intercept = normal(0.0, 1.0) @ "intercept"
    # ... モデルの残り
```

## なぜこれが重要なのか

従来のプロバビリスティック・プログラミングシステムは、推論の制御が限られていました：低速な汎用アルゴリズムを受け入れるか、すべてを一から実装するかの二択でした。Genの**プログラマブル推論**アプローチでは、アルゴリズムのコンポーネント—重要度サンプリング、MCMCカーネル、勾配ベースの手法—を合成して、モデルの構造に適した推論戦略を構築できます。

これは特に以下の場合に価値があります：

- 無限次元パラメータ空間を持つ**ベイズノンパラメトリックモデル**
- 潜在変数の数自体が未知である**構造化予測**
- 解釈可能な不確実性定量化を必要とする**科学的モデリング**
- 3Dシーンについて推論する必要がある**ロボティクスと知覚**システム

## 実践例：ベイズ住宅価格予測

GenJAXを実際の問題に適用してみましょう：[Kaggle House Pricesデータセット](https://www.kaggle.com/competitions/house-prices-advanced-regression-techniques)を使用した住宅価格予測です。このデータセットには、アイオワ州エイムズの住宅を記述する79の特徴量が含まれており、`SalePrice`を予測することが目標です。

従来の回帰による点推定の代わりに、**不確実性定量化**を提供するベイズモデルを構築します—予測価格が*いくら*かだけでなく、その予測に*どれだけ確信を持てるか*も知ることができます。

### 問題設定

Kaggleデータセットには以下のような特徴量が含まれています：
- `GrLivArea`：地上居住面積（平方フィート）
- `OverallQual`：全体的な材質と仕上げの品質（1-10）
- `YearBuilt`：最初の建設年
- `TotalBsmtSF`：地下室の総面積

簡単のため、数値特徴量のサブセットに焦点を当て、ベイズ線形回帰モデルを構築します。

### 生成モデルの定義

GenJAXでは、住宅価格がどのように生成されるかについての信念を表現します：

```python
import jax
import jax.numpy as jnp
import jax.random as jrandom
from genjax import gen, normal, uniform, Target, ChoiceMap
from genjax.inference.smc import ImportanceK

@gen
def house_price_model(X):
    """
    住宅価格のベイズ線形回帰。
    X: 特徴量行列 (n_samples, n_features)
    """
    # 回帰係数の事前分布
    # 品質/サイズ特徴量には正の係数を期待
    coef_0 = normal(0.0, 1.0) @ "coef_0"
    coef_1 = normal(0.0, 1.0) @ "coef_1"
    coef_2 = normal(0.0, 1.0) @ "coef_2"
    coef_3 = normal(0.0, 1.0) @ "coef_3"

    # 切片の事前分布（対数スケール、対数価格を予測するため）
    intercept = normal(12.0, 1.0) @ "intercept"  # 約$160kのベースライン

    # ノイズの事前分布（同じ特徴量でも住宅価格は変動する）
    noise_std = uniform(0.1, 0.5) @ "noise_std"

    # 各住宅の予測を生成
    coeffs = jnp.array([coef_0, coef_1, coef_2, coef_3])
    predictions = X @ coeffs + intercept

    # 尤度：線形モデルが与えられた場合の観測価格
    # GenJAXはベクトル化された分布をサポート - 正規分布は
    # predictions配列全体にブロードキャストし、各要素を独立として扱う
    log_prices = normal(predictions, noise_std) @ "log_prices"

    return predictions
```

このモデルはいくつかの仮定をエンコードしています：
1. **係数は不確実**：真の関係がわからないため、事前分布を設定
2. **価格は対数正規**：価格は負になれず、右に歪んでいる（対数価格をモデル化することで、乗法的効果が加法的になる）
3. **異質なノイズ**：類似の住宅でも異なる価格で売れる

> **注**：このモデルは、JAX互換性のために明示的な係数変数とベクトル化された尤度を使用しています。ベクトル化された`normal(predictions, noise_std)`は全サンプルに効率的にブロードキャストします。完全な実装については[完全なサンプルコード]([../src/house_price_genjax.pyi](https://github.com/thebongcook/genjax-intro/blob/main/src/house_price_genjax.py))を参照してください。

### 推論の実行

観測された販売価格が与えられたとき、係数の事後分布を推論したいです：

```python
import pandas as pd
import numpy as np

# データの読み込みと前処理
train_df = pd.read_csv("train.csv")
features = ["GrLivArea", "OverallQual", "YearBuilt", "TotalBsmtSF"]

X = train_df[features].fillna(0).values.astype(np.float32)
y = np.log(train_df["SalePrice"].values).astype(np.float32)

# 特徴量の標準化
X_mean, X_std = X.mean(axis=0), X.std(axis=0)
X = (X - X_mean) / X_std

# JAX配列に変換
X_train = jnp.array(X)
y_train = y

# 観測価格を使用して制約マップを構築（ベクトル化）
constraints = ChoiceMap.d({"log_prices": y_train})

# 推論ターゲットの作成
target = Target(
    house_price_model,
    (X_train,),
    constraints
)

# 複数のパーティクルで重要度サンプリングを実行
key = jrandom.PRNGKey(42)
k_particles = 1000
alg = ImportanceK(target, k_particles=k_particles)
sub_keys = jrandom.split(key, k_particles)
_, posterior_samples = jax.vmap(alg.random_weighted, in_axes=(0, None))(sub_keys, target)
```

### 得られるもの：予測だけでなく不確実性

このアプローチの美しさは、単一の係数推定値ではなく**分布**が得られることです：

```python
# 係数の事後サンプルを抽出
coef_samples = posterior_samples["coef_0"]  # GrLivArea係数
```

1,000の重要度サンプリングパーティクル（70/30の訓練/テスト分割、1,021の訓練サンプルと439のテストサンプル）でKaggleデータセットに対して[完全なサンプル](../src/house_price_genjax.py)を実行すると、以下の結果が得られます：

```
係数の事後推定値：
--------------------------------------------------
    GrLivArea      :   0.116 ± 0.247  (95% CI: [-0.346, 0.588])
    OverallQual    :   0.174 ± 0.284  (95% CI: [-0.395, 0.729])
    YearBuilt      :   0.091 ± 0.236  (95% CI: [-0.379, 0.542])
    TotalBsmtSF    :   0.043 ± 0.240  (95% CI: [-0.395, 0.513])
    Intercept      :  12.026 ± 0.197
    Noise Std      :   0.398 ± 0.075

ホールドアウト予測と不確実性（テストセット）：
--------------------------------------------------
    439件のホールドアウト予測のうち10件を表示：
    住宅  | 実際の価格    | 予測（平均）    | 90%信頼区間
    -----------------------------------------------------------------
        1 | $    112,000 | $        97,850 | $    34,101 - $   271,070
        2 | $    123,600 | $       121,401 | $    41,657 - $   355,248
        3 | $    240,000 | $       161,917 | $    47,347 - $   497,824
        4 | $    117,000 | $       116,189 | $    51,273 - $   263,998
        5 | $    281,213 | $       291,776 | $   105,307 - $   835,386
        6 | $    275,500 | $       266,397 | $    99,238 - $   685,142
        7 | $    130,000 | $       122,954 | $    54,602 - $   296,081
        8 | $    345,000 | $       322,223 | $   117,288 - $   957,747
        9 | $    123,000 | $       125,042 | $    54,339 - $   300,636
       10 | $    179,200 | $       187,161 | $    82,738 - $   448,681

テストセットのサマリー指標（全439サンプル）：
--------------------------------------------------
    平均絶対誤差（対数価格）: 0.1209
    平均絶対誤差（実際の価格）: $21,535
    90%信頼区間のカバレッジ: 99.8%
```

信頼区間（CI）が実際の価格を捉えていることに注目してください—モデルが一度も見たことのないホールドアウトデータでさえもです。信頼区間の高いカバレッジは、モデルが意思決定において単一の点推定よりもはるかに有用な、正直な不確実性定量化を提供していることを示しています。

#### なぜ不確実性が意思決定を変えるのか

結果から住宅134と住宅135を比較してみましょう：

| 住宅 | 予測値 | 90% CI | CI幅 |
|------|--------|--------|------|
| 134 | $349,954 | $141k - $793k | $651k |
| 135 | $347,868 | $97k - $1,302k | $1,204k |

点推定モデルでは、両方の住宅が約$350kの価値があると言うでしょう—事実上同一です。しかし、不確実性定量化があれば、意思決定者はまったく異なる状況を見ることになります：

**住宅ローンの貸し手**：住宅135の場合、広い区間（$97k-$1.3M）は高い評価の不確実性を示しています—モデルの不確実性は、訓練データ内の類似住宅に対して観測された特徴量がどの程度価格を説明できるかを反映しています。貸し手は標準の20%ではなく25%の頭金を要求したり、追加の鑑定を依頼したりするかもしれません。住宅134では、より狭い区間がより高い確信を提供し、予測価格が同じであっても標準的な条件を許可できます。

**住宅購入者**（ここでは購入者がデータに精通しており、近隣の住宅データに十分にアクセスできると仮定します）：両方の住宅が$380k（予測より約9%高い）で売りに出されている場合、不確実性は異なる物語を語ります。住宅134では、より狭いCIにより、$380kは妥当な範囲内に十分収まっています—進めても問題ありません。住宅135では、広いCI（$97k-$1.3M）により、同じ$380kの希望価格を評価するのが難しくなります—真の価値はもっと低い可能性も*あれば*、$380kで実際にお買い得である可能性もあります。いずれにせよ、購入者は契約前にさらに調査すべきです。

**不動産投資家**：貸し手にとってのリスクは、投資家にとっての機会となりえます。住宅135の広い不確実性は、モデルが確信を持てない物件であることを示しています—おそらく裁定取引の機会を生み出すユニークな特性によるものです。投資家は、この住宅がミスプライスされているかどうかを調べるために優先的に調査するかもしれません。一方、住宅134のより狭い範囲は、市場がおそらく正しく価格設定していることを示唆しています。

重要な洞察：**同一の予測価格でも、異なる不確実性の範囲は異なる行動につながるべきです**。点推定はこの重要な情報を隠してしまいます。

### モデルの拡張：外れ値の処理

不動産データにはしばしば外れ値があります。例えば、良い立地にある2,000平方フィートの住宅が、モデルでは$450,000で売れると予測されるのに、差し押さえ物件だったために実際には$180,000で売却されたケースを想像してみてください。外れ値処理がなければ、この1つのデータポイントが係数を真の値から引き離し、すべての通常の販売に対する予測精度を低下させてしまいます。

従来のワークフローでは、新しい推論方程式を一から導出する必要がありました。しかしGenJAXでは、確率的分岐を使ってモデルを簡単に拡張できます：

```python
from genjax import switch

# 通常の住宅：予測周りの狭いガウス分布
@gen
def inlier_obs(mean, noise_std):
    return normal(mean, noise_std) @ "obs"

# 外れ値：広い一様分布
@gen
def outlier_obs(mean, noise_std):
    return uniform(10.0, 14.0) @ "obs"

# switchはインデックスで分岐を選択：0 → inlier_obs、1 → outlier_obs
obs_model = switch(inlier_obs, outlier_obs)

@gen
def robust_house_price_model(X):
    # ... 係数と切片は以前と同様 ...

    for i in range(X.shape[0]):
        # この住宅は外れ値か？
        is_outlier = flip(0.05) @ f"outlier_{i}"
        # switchは整数インデックスが必要；flipはboolを返す
        idx = is_outlier.astype(jnp.int32)
        log_price = obs_model(idx, (predictions[i], noise_std), (predictions[i], noise_std)) @ f"y_{i}"
```

ここがGenのプログラマブル推論が輝くところです—離散的な外れ値インジケータにはギブスサンプリングを、連続的な係数にはハミルトニアンモンテカルロ（HMC）を混合できます。

## 主なポイント

- **実用的な適用性**：住宅価格の例で示したように、GenJAXは正直な不確実性推定を提供しながら実際の問題に取り組めます
- **プログラマブル推論**：Genは数学を自動化しながらアルゴリズムを制御できます—必要に応じて重要度サンプリングとMCMCカーネルを組み合わせ可能
- **不確実性定量化**：点推定の代わりに、信頼区間を持つ完全な事後分布が得られます—実世界の意思決定にはるかに有用
- **表現力豊かなモデリング**：確率的分岐を通じて外れ値検出のような複雑なシナリオを処理し、離散的推論と連続的推論を混合

## 結論

Genは、プロバビリスティック・プログラミングの考え方の転換を表しています：推論をブラックボックスとして扱う代わりに、実務者にモデルに合わせたカスタムアルゴリズムを構築するツールを提供します。GenJAXにより、これらの機能がJAXのパフォーマンス上の利点とともに、より広いPython/AIコミュニティに利用可能になりました。

科学研究のためのベイズモデルを構築する場合でも、不確実性の下で推論する必要がある知覚システムを開発する場合でも、GenJAXは柔軟性と自動化の間の魅力的な中間点を提供します。Gaudiyでは、AIエージェントに重点を置いており、プロバビリスティック・プログラミングはそのようなシステムにおける不確実性を定量化するための原理的なフレームワークを提供し、キャリブレーションとロバスト性の向上に役立ちます。

## 参考文献

- [Gen公式ウェブサイト](https://www.gen.dev/)
- [GenJAXドキュメント](https://genjax.gen.dev/)
- [GenJAX GitHubリポジトリ](https://github.com/genjax-community/genjax)
- [Gen.jl GitHubリポジトリ](https://github.com/probcomp/Gen.jl)
- [MITプロバビリスティック・コンピューティングプロジェクト](https://github.com/probcomp)
- [Genチュートリアル](https://www.gen.dev/tutorials/)
- [確率とプロバビリスティック・コンピューティングチュートリアル](https://josephausterweil.github.io/probintro/intro/index.html)
- [Kaggle House Pricesコンペティション](https://www.kaggle.com/competitions/house-prices-advanced-regression-techniques)
- [Cusumano-Towner, M. F., Saad, F. A., Lew, A., & Mansinghka, V. K. (2019). Gen: A General-Purpose Probabilistic Programming System with Programmable Inference. PLDI 2019.](https://www.cs.cmu.edu/~fsaad/assets/papers/2019-CusumanoTownerEtAl-PLDI.pdf)
