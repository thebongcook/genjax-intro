# GenJAX: プログラマブル推論を備えたプロバビリスティック・プログラミング

## はじめに

プロバビリスティック・プログラミングは、不確実性の下で推論を行うモデルを構築するための強力なパラダイムとして台頭してきました。従来の確率的機械学習では、実務者は通常、推論アルゴリズムを手作業で設計していました—これは深い数学的専門知識を必要とする、労力がかかりエラーを起こしやすいプロセスです。プロバビリスティック・プログラミングはこれを変えます：複雑な推論アルゴリズムを手作業でコーディングする代わりに、実務者はモデルをプログラムとして表現し、数学的な重労働をシステムに任せることができます。しかし、ここに課題があります：汎用的な推論は往々にして遅すぎる一方で、手作業でチューニングしたアルゴリズムは実装と保守が煩雑です。

そこで登場するのが**Gen**です。MITのプロバビリスティック・コンピューティングプロジェクトによって開発されたこのプロバビリスティック・プログラミングシステムは、表現力豊かなモデリングとプログラマブルな推論という両方の利点を提供します。

## Genとは何か？

Genは、推論アルゴリズムの難解な数学と低レベルの実装詳細を自動化しながら、ユーザーに特定のモデルに合わせたソリューションをカスタマイズする柔軟性を与えるオープンソースのプロバビリスティック・プログラミングシステムです。

もともとJuliaで構築された（[Gen.jl](https://github.com/probcomp/Gen.jl)として）Genは、確率的分岐、ループ、再帰を含む通常のコードを使用して生成モデルを記述でき、効率的な推論アルゴリズムを構築するための構成要素を提供します。

### 主な機能：

- **ハイブリッド推論**：ニューラルネットワークの提案分布と原理的なベイズ推論（マルコフ連鎖モンテカルロ（MCMC）、逐次モンテカルロ（SMC）、変分推論）を単一のワークフローでシームレスに組み合わせ
- **確率的構造**：潜在変数の数が未知である問題（例：物体検出、未知のKを持つクラスタリング）をモデル化
- **プログラマブルなトレードオフ**：プロトタイピングには高レベルの抽象化を使用し、本番環境では最適化されたコードにドロップダウン
- **クリーンなAPI**：自動微分と推論プリミティブを利用

## GenJAXの登場：Python向けGPUアクセラレーテッドベイズ推論

**GenJAX**は、GenのコアとなるアブストラクションをJAX上で直接再実装し、本番AIが求めるパフォーマンス特性を持つ原理的な不確実性定量化をPythonにもたらします。すでにPythonエコシステムで作業している実務者にとって、これはスタックを離れることなくプログラマブルな推論が可能になることを意味します—JAXの強みを活用して：

- **GPUアクセラレーション**：推論計算のJITコンパイルとGPUデバイスへの自動バッチ処理
- **JAX変換**：`jit`、`vmap`、自動微分との完全な互換性
- **馴染みのある構文**：`@gen`デコレータを使用してPythonで生成関数を定義
- **組み込みアルゴリズム**：サンプリング重要度リサンプリング（SIR）やその他の推論手法をすぐに利用可能

### クイックサンプル

GenJAXでは、生成関数（構造化されたサンプル空間上の確率測度を表す計算オブジェクト）を定義します。これらの関数は合成、トレース、様々な推論アルゴリズムとの使用が可能です：

```python
@gen
def my_model():
    slope = normal(0.0, 1.0) @ "slope"
    intercept = normal(0.0, 1.0) @ "intercept"
    # ... モデルの残り
```

## なぜこれが重要なのか

確率的推論への従来のアプローチは、トレードオフを強いてきました：ニューラルネットワークは高速ですが新規データで失敗する可能性があり、一方でモデルベースの推論は正確ですが計算コストが高くなります。GenJAXは、学習済みの提案分布を使用して原理的なベイズ推論を加速する**ハイブリッドアプローチ**を可能にします。

これは特に以下の場合に価値があります：

- 無限次元パラメータ空間を持つ**ベイズノンパラメトリックモデル**
- 潜在変数の数自体が未知である**構造化予測**
- 解釈可能な不確実性定量化を必要とする**科学的モデリング**
- 3Dシーンについて推論する必要がある**ロボティクスと知覚**システム

## 実践例：ベイズ住宅価格予測

GenJAXを実際の問題に適用してみましょう：[Kaggle House Pricesデータセット](https://www.kaggle.com/competitions/house-prices-advanced-regression-techniques)を使用した住宅価格予測です。このデータセットには、アイオワ州エイムズの住宅を記述する79の特徴量が含まれており、`SalePrice`を予測することが目標です。

従来の回帰による点推定の代わりに、**不確実性定量化**を提供するベイズモデルを構築します—予測価格が*いくら*かだけでなく、その予測に*どれだけ確信を持てるか*も知ることができます。

### 問題設定

Kaggleデータセットには以下のような特徴量が含まれています：
- `GrLivArea`：地上居住面積（平方フィート）
- `OverallQual`：全体的な材質と仕上げの品質（1-10）
- `YearBuilt`：最初の建設年
- `TotalBsmtSF`：地下室の総面積

簡単のため、数値特徴量のサブセットに焦点を当て、ベイズ線形回帰モデルを構築します。

### 生成モデルの定義

GenJAXでは、住宅価格がどのように生成されるかについての信念を表現します：

```python
import jax
import jax.numpy as jnp
import jax.random as jrandom
from genjax import gen, normal, uniform, Target, ChoiceMap
from genjax.inference.smc import ImportanceK

@gen
def house_price_model(X):
    """
    住宅価格のベイズ線形回帰。
    X: 特徴量行列 (n_samples, n_features)
    """
    # 回帰係数の事前分布
    # 品質/サイズ特徴量には正の係数を期待
    coef_0 = normal(0.0, 1.0) @ "coef_0"
    coef_1 = normal(0.0, 1.0) @ "coef_1"
    coef_2 = normal(0.0, 1.0) @ "coef_2"
    coef_3 = normal(0.0, 1.0) @ "coef_3"

    # 切片の事前分布（対数スケール、対数価格を予測するため）
    intercept = normal(12.0, 1.0) @ "intercept"  # 約$160kのベースライン

    # ノイズの事前分布（同じ特徴量でも住宅価格は変動する）
    noise_std = uniform(0.1, 0.5) @ "noise_std"

    # 各住宅の予測を生成
    coeffs = jnp.array([coef_0, coef_1, coef_2, coef_3])
    predictions = X @ coeffs + intercept

    # 尤度：線形モデルが与えられた場合の観測価格
    # GenJAXはベクトル化された分布をサポート - 正規分布は
    # predictions配列全体にブロードキャストし、各要素を独立として扱う
    log_prices = normal(predictions, noise_std) @ "log_prices"

    return predictions
```

このモデルはいくつかの仮定をエンコードしています：
1. **係数は不確実**：真の関係がわからないため、事前分布を設定
2. **価格は対数正規**：価格は負になれず、右に歪んでいる（対数価格をモデル化することで、乗法的効果が加法的になる）
3. **異質なノイズ**：類似の住宅でも異なる価格で売れる

> **注**：このモデルは、JAX互換性のために明示的な係数変数とベクトル化された尤度を使用しています。ベクトル化された`normal(predictions, noise_std)`は全サンプルに効率的にブロードキャストします。完全な実装については[完全なサンプルコード]([../src/house_price_genjax.pyi](https://github.com/thebongcook/genjax-intro/blob/main/src/house_price_genjax.py))を参照してください。

### 推論の実行

観測された販売価格が与えられたとき、係数の事後分布を推論したいです：

```python
import pandas as pd
import numpy as np

# データの読み込みと前処理
train_df = pd.read_csv("train.csv")
features = ["GrLivArea", "OverallQual", "YearBuilt", "TotalBsmtSF"]

X = train_df[features].fillna(0).values.astype(np.float32)
y = np.log(train_df["SalePrice"].values).astype(np.float32)

# 特徴量の標準化
X_mean, X_std = X.mean(axis=0), X.std(axis=0)
X = (X - X_mean) / X_std

# JAX配列に変換
X_train = jnp.array(X)
y_train = y

# 観測価格を使用して制約マップを構築（ベクトル化）
constraints = ChoiceMap.d({"log_prices": y_train})

# 推論ターゲットの作成
target = Target(
    house_price_model,
    (X_train,),
    constraints
)

# 複数のパーティクルで重要度サンプリングを実行
key = jrandom.PRNGKey(42)
k_particles = 1000
alg = ImportanceK(target, k_particles=k_particles)
sub_keys = jrandom.split(key, k_particles)
_, posterior_samples = jax.vmap(alg.random_weighted, in_axes=(0, None))(sub_keys, target)
```

### 得られるもの：予測だけでなく不確実性

このアプローチの美しさは、単一の係数推定値ではなく**分布**が得られることです：

```python
# 係数の事後サンプルを抽出
coef_samples = posterior_samples["coef_0"]  # GrLivArea係数
```

1,000の重要度サンプリングパーティクル（70/30の訓練/テスト分割、1,021の訓練サンプルと439のテストサンプル）でKaggleデータセットに対して[完全なサンプル](../src/house_price_genjax.py)を実行すると、以下の結果が得られます：

```
係数の事後推定値：
--------------------------------------------------
    GrLivArea      :   0.116 ± 0.247  (95% CI: [-0.346, 0.588])
    OverallQual    :   0.174 ± 0.284  (95% CI: [-0.395, 0.729])
    YearBuilt      :   0.091 ± 0.236  (95% CI: [-0.379, 0.542])
    TotalBsmtSF    :   0.043 ± 0.240  (95% CI: [-0.395, 0.513])
    Intercept      :  12.026 ± 0.197
    Noise Std      :   0.398 ± 0.075

ホールドアウト予測と不確実性（テストセット）：
--------------------------------------------------
    439件のホールドアウト予測のうち10件を表示：
    住宅  | 実際の価格    | 予測（平均）    | 90%信頼区間
    -----------------------------------------------------------------
        1 | $    112,000 | $        97,850 | $    34,101 - $   271,070
        2 | $    123,600 | $       121,401 | $    41,657 - $   355,248
        3 | $    240,000 | $       161,917 | $    47,347 - $   497,824
        4 | $    117,000 | $       116,189 | $    51,273 - $   263,998
        5 | $    281,213 | $       291,776 | $   105,307 - $   835,386
        6 | $    275,500 | $       266,397 | $    99,238 - $   685,142
        7 | $    130,000 | $       122,954 | $    54,602 - $   296,081
        8 | $    345,000 | $       322,223 | $   117,288 - $   957,747
        9 | $    123,000 | $       125,042 | $    54,339 - $   300,636
       10 | $    179,200 | $       187,161 | $    82,738 - $   448,681

テストセットのサマリー指標（全439サンプル）：
--------------------------------------------------
    平均絶対誤差（対数価格）: 0.1209
    平均絶対誤差（実際の価格）: $21,535
    90%信頼区間のカバレッジ: 99.8%
```

信頼区間（CI）が実際の価格を捉えていることに注目してください—モデルが一度も見たことのないホールドアウトデータでさえもです。信頼区間の高いカバレッジは、モデルが意思決定において単一の点推定よりもはるかに有用な、正直な不確実性定量化を提供していることを示しています。

### モデルの拡張：外れ値の処理

不動産データにはしばしば外れ値があります。例えば、良い立地にある2,000平方フィートの住宅が、モデルでは$450,000で売れると予測されるのに、差し押さえ物件だったために実際には$180,000で売却されたケースを想像してみてください。外れ値処理がなければ、この1つのデータポイントが係数を真の値から引き離し、すべての通常の販売に対する予測精度を低下させてしまいます。

従来のワークフローでは、新しい推論方程式を一から導出する必要がありました。しかしGenJAXでは、確率的分岐を使ってモデルを簡単に拡張できます：

```python
@gen
def robust_house_price_model(X):
    # ... 係数と切片は以前と同様 ...

    for i in range(X.shape[0]):
        # この住宅は外れ値か？
        is_outlier = flip(0.05) @ f"outlier_{i}"

        if is_outlier:
            # 外れ値：広い一様分布
            log_price = uniform(10.0, 14.0) @ f"log_price_{i}"
        else:
            # 通常の住宅：予測周りの狭いガウス分布
            log_price = normal(predictions[i], noise_std) @ f"log_price_{i}"
```

ここがGenのプログラマブル推論が輝くところです—離散的な外れ値インジケータにはギブスサンプリングを、連続的な係数にはハミルトニアンモンテカルロ（HMC）を混合できます。

## 主なポイント

- **実用的な適用性**：住宅価格の例で示したように、GenJAXは正直な不確実性推定を提供しながら実際の問題に取り組めます
- **プログラマブル推論**：Genは数学を自動化しながらアルゴリズムを制御できます—必要に応じて重要度サンプリング、MCMC、ニューラル提案を組み合わせ可能
- **不確実性定量化**：点推定の代わりに、信頼区間を持つ完全な事後分布が得られます—実世界の意思決定にはるかに有用
- **表現力豊かなモデリング**：確率的分岐を通じて外れ値検出のような複雑なシナリオを処理し、離散的推論と連続的推論を混合

## 結論

Genは、プロバビリスティック・プログラミングの考え方の転換を表しています：推論をブラックボックスとして扱う代わりに、実務者にモデルに合わせたカスタムアルゴリズムを構築するツールを提供します。GenJAXにより、これらの機能がJAXのパフォーマンス上の利点とともに、より広いPython/AIコミュニティに利用可能になりました。

科学研究のためのベイズモデルを構築する場合でも、不確実性の下で推論する必要がある知覚システムを開発する場合でも、GenJAXは柔軟性と自動化の間の魅力的な中間点を提供します。Gaudiyでは、AIエージェントに重点を置いており、プロバビリスティック・プログラミングはそのようなシステムにおける不確実性を定量化するための原理的なフレームワークを提供し、キャリブレーションとロバスト性の向上に役立ちます。

## 参考文献

- [Gen公式ウェブサイト](https://www.gen.dev/)
- [GenJAXドキュメント](https://genjax.gen.dev/)
- [GenJAX推論API](https://genjax.gen.dev/library/inference.html)
- [Gen.jl GitHubリポジトリ](https://github.com/probcomp/Gen.jl)
- [MITプロバビリスティック・コンピューティングプロジェクト](https://github.com/probcomp)
- [Genチュートリアル](https://www.gen.dev/tutorials/)
- [確率とプロバビリスティック・コンピューティングチュートリアル](https://josephausterweil.github.io/probintro/intro/index.html)
- [Kaggle House Pricesコンペティション](https://www.kaggle.com/competitions/house-prices-advanced-regression-techniques)
- [Cusumano-Towner, M. F., Saad, F. A., Lew, A., & Mansinghka, V. K. (2019). Gen: A General-Purpose Probabilistic Programming System with Programmable Inference. PLDI 2019.](https://www.cs.cmu.edu/~fsaad/assets/papers/2019-CusumanoTownerEtAl-PLDI.pdf)
